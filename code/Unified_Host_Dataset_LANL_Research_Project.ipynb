{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANL Host Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article : https://www.worldscientific.com/doi/pdf/10.1142/9781786345646_001\n",
    "\n",
    "Notes:\n",
    "\n",
    "\"The events from the host logs included in the data set are all related to authentication and process activity on each machine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour obtenir le dataset:\n",
    "1- https://csr.lanl.gov/data/2017/#citing : donner mail, obtenir le lien\n",
    "\n",
    "2- https://csr.lanl.gov/data-fence/... 10 chiffres.../... token... iXYXXbqw15UugRnZALCZ2Y8dvEk=... /unified-host-network-dataset-2017/wls.html pour avoir l'index avec tous les fichiers compressés\n",
    "\n",
    "3- download : for i in $(seq -w 1 90); do wget -c https://csr.lanl.gov/data-fence/...10 chiffres.../...token.../unified-host-network-dataset-2017/wls/wls_day-$i.bz2; done\n",
    "\n",
    "4- decompress, as required : bzip2 -dk filename.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EventID : \n",
    "\n",
    "EventID         Description\n",
    "\n",
    "Authentication events\n",
    "\n",
    "4768            Kerberos authentication ticket was requested (TGT)\n",
    "\n",
    "4769            Kerberos service ticket was requested (TGS)\n",
    "\n",
    "4770            Kerberos service ticket was renewed\n",
    "\n",
    "4774            An account was mapped for logon\n",
    "\n",
    "4776            Domain controller attempted to validate credentials\n",
    "\n",
    "4624            An account successfully logged on, see Logon Types\n",
    "\n",
    "4625            An account failed to logon, see Logon Types\n",
    "\n",
    "4634            An account was logged off, see Logon Types\n",
    "\n",
    "4647            User initiated logoff\n",
    "\n",
    "4648            A logon was attempted using explicit credentials\n",
    "\n",
    "4672            Special privileges assigned to a new logon\n",
    "\n",
    "4800            The workstation was locked\n",
    "\n",
    "4801            The workstation was unlocked\n",
    "\n",
    "4802            The screensaver was invoked\n",
    "\n",
    "4803            The screensaver was dismissed\n",
    "\n",
    "Process events\n",
    "\n",
    "4688            Process start\n",
    "\n",
    "4689            Process end\n",
    "\n",
    "System events\n",
    "\n",
    "4608            Windows is starting up\n",
    "\n",
    "4609            Windows is shutting down\n",
    "\n",
    "1100            Event logging service has shut down (often recorded instead of EventID 4609)\n",
    "\n",
    "\n",
    "Detailed description : - EventID : https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/appendix-l--events-to-monitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logon Types for EventIDs: 4624, 4625 and 4634\n",
    "\n",
    "LogonTypes (EventIDs: 4624, 4625 and 4634)\n",
    "\n",
    "2 — Interactive\n",
    "\n",
    "3 — Network\n",
    "\n",
    "4 — Batch\n",
    "\n",
    "12 — Cached Remote-Interactive\n",
    "\n",
    "5 — Service\n",
    "\n",
    "9 — New Credentials\n",
    "\n",
    "7 — Unlock\n",
    "\n",
    "10 — Remote Interactive\n",
    "\n",
    "8 — Network Clear Text 11 — Cached Interactive\n",
    "\n",
    "0 — Used only by the system account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host Log Fields\n",
    "\n",
    ". Time: The epoch time of the event in seconds.\n",
    "\n",
    "• EventID: Four digit integer corresponding to the event id of the record.\n",
    "\n",
    "• LogHost: The hostname of the computer that the event was recorded on.In the case of directed authentication events, the LogHost will correspond to the computer that the authentication event is terminating at (destination computer).\n",
    "\n",
    "• LogonType: Integer corresponding to the type of logon, see Table 2.\n",
    "\n",
    "• LogonTypeDescription: Description of the LogonType, see Table 2.\n",
    "\n",
    "• UserName: The user account initiating the event. If the user ends in $, then it corresponds to a computer account for the specified computer.\n",
    "\n",
    "• DomainName: Domain name of UserName.\n",
    "\n",
    "• LogonID: A semi-unique (unique between current sessions and LogHost)number that identifies the logon session just initiated. Any events logged subsequently during this logon session should report the same LogonID through to the logoff event.\n",
    "\n",
    "• SubjectUserName: For authentication mapping events, the user account specified by this field is mapping to the user account in UserName.\n",
    "\n",
    "• SubjectDomainName: Domain name of SubjectUserName.\n",
    "\n",
    "• SubjectLogonID: See LogonID.\n",
    "\n",
    "• Status: Status of the authentication request. “0 × 0” means success otherwise failure; failure codes for the appropriate EventID are available online.f\n",
    "\n",
    "• Source: For authentication events, this will correspond to the the computer where the authentication originated (source computer), if it is a local logon event then this will be the same as the LogHost.\n",
    "\n",
    "• ServiceName: The account name of the computer or service the user is requesting the ticket for.\n",
    "\n",
    "• Destination: This is the server the mapped credential is accessing. This may indicate the local computer when starting another process with new account credentials on a local computer.\n",
    "\n",
    "• AuthenticationPackage: The type of authentication occurring including Negotiate, Kerberos, NTLM plus a few more.\n",
    "\n",
    "• FailureReason: The reason for a failed logon.\n",
    "\n",
    "• ProcessName: The process executable name, for authentication events this is the process that processed the authentication event. ProcessNames may include the file type extensions (i.e., exe).\n",
    "\n",
    "• ProcessID: A semi-unique (unique between currently running processes AND LogHost) value that identifies the process. ProcessID allows you to correlate other events logged in association with the same process through to the process end.\n",
    "\n",
    "• ParentProcessName: The process executable that started the new process. ParentProcessNames often do not have file extensions like ProcessName but can be compared by removing file extensions from the name.\n",
    "\n",
    "• ParentProcessID: Identifies the exact process that started the new process. Look for a preceding event 4688 with a ProcessID that matches this ParentProcessID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:59:23.159254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from sklearn.svm import OneClassSVM\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from sklearn.mixture import BayesianGaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du nombre de lignes des deux premiers fichiers wls_day-01.bz2 et wls_day-02.bz2 une fois décompressés : plus de 50 millions de lignes par fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affiche le nombre de lignes de chaque fichier\n",
    "\n",
    "def get_lines_number(filepath):\n",
    "    \"\"\"retourne le nombre de lignes d'un fichier *.json\n",
    "\n",
    "    Args:\n",
    "        filename (_type_): full path filename\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        ctr_lignes = sum(1 for line in f)\n",
    "        \n",
    "    return ctr_lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/benjamin.deporte/Cyber/code\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "if os.getcwd()=='/home/benjamin.deporte/Cyber/code':\n",
    "    DIRPATH='/home/benjamin.deporte/Cyber/data/LALN_processed/'\n",
    "else:\n",
    "    DIRPATH='/home/benjamin/Folders_Python/Cyber/data/LALN_processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = DIRPATH\n",
    "liste_fichiers = [ 'wls_day-01.json', 'wls_day-02.json']\n",
    "\n",
    "for filename in liste_fichiers:\n",
    "    filepath = dirpath + filename\n",
    "\n",
    "    # print(f'{filename} contient {get_lines_number(filepath)} lignes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework pour le traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE DES DONNEES\n",
    "# ---------------------\n",
    "\n",
    "# il y a 90 fichiers wls_day-nn.bz2, un pour chaque jour d'enregistrement\n",
    "\n",
    "# chaque fichier wls_day-nn.bz2 peut se décompresser :\n",
    "# - via la ligne de commande : bzip2 -dk filename.bz2 (préserve le fichier compressé original)\n",
    "# - via la bibliothèque python : https://docs.python.org/3/library/bz2.html\n",
    "\n",
    "# un fichier décompressé est un fichier *.json au format lignes : chaque ligne du fichier est un objet JSON distinct\n",
    "\n",
    "# chaque objet JSON est un objet à un seul niveau, avec un maximum de 21 paires (key, value)\n",
    "\n",
    "# ALGORITHME\n",
    "# ----------\n",
    "\n",
    "# on va lire les objets JSON un par un (ligne par ligne) et traiter les données séquentiellement.\n",
    "# chaque champ est managé par un dictionnaire, où :\n",
    "# - les keys sont les valeurs possibles, déjà vues, du champ\n",
    "# - les values sont une liste de deux int : 1/ une valeur pour encoder categorical, 2/ un compteur\n",
    "\n",
    "# PROCESS :\n",
    "# ---------\n",
    "\n",
    "# loop sur objets JSON :\n",
    "# - concaténation de UserName et DomaineName, pour donner un UserNameDomainName qui identifie le user account. Drop UserName, DomaineName\n",
    "# - pour chacun des 20 champs :\n",
    "# -- checke si présent dans le JSON object\n",
    "# ---- si non présent : valeur = -1 (correspond à NaN)\n",
    "# ---- si présent : regarde dans le dictionnaire du champ\n",
    "# ------ si existe déjà : retourne valeur, incrémente compteur\n",
    "# ------ si n'existe pas : crée key, crée nouvelle valeur (= max anciennes + 1), met compteur à 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des champs à traiter\n",
    "\n",
    "list_fields = [\n",
    "    'Time', # int64\n",
    "    'EventID', # int64\n",
    "    'LogHost', # object\n",
    "    'LogonType', # float64, corresponding to the type of logon, see Table 2.\n",
    "    'LogonTypeDescription', # object Description of the LogonType, see Table 2.\n",
    "    'UserName', # object - The user account initiating the event. If the user ends in $, then it corresponds to a computer account for the specified computer.\n",
    "    'DomainName', # object - Domain name of UserName.\n",
    "    'LogonID', #: object. A semi-unique (unique between current sessions and LogHost)number that identifies the logon session just initiated. Any events logged subsequently during this logon session should report the same LogonID through to the logoff event.\n",
    "    'SubjectUserName', # object. For authentication mapping events, the user account specified by this field is mapping to the user account in UserName.\n",
    "    'SubjectDomainName', # object - Domain name of SubjectUserName.\n",
    "    'SubjectLogonID', # object - See LogonID.\n",
    "    'Status', # object - Status of the authentication request. “0 × 0” means success otherwise failure; failure codes for the appropriate EventID are available online.f\n",
    "    'Source', # object - For authentication events, this will correspond to the the computer where the authentication originated (source computer), if it is a local logon event then this will be the same as the LogHost.\n",
    "    'ServiceName', # object - The account name of the computer or service the user is requesting the ticket for.\n",
    "    'Destination', # object - This is the server the mapped credential is accessing. This may indicate the local computer when starting another process with new account credentials on a local computer.\n",
    "    'AuthenticationPackage', # object - The type of authentication occurring including Negotiate, Kerberos, NTLM plus a few more.\n",
    "    'FailureReason', # object - The reason for a failed logon.\n",
    "    'ProcessName', # object - The process executable name, for authentication events this is the process that processed the authentication event. ProcessNames may include the file type extensions (i.e., exe).\n",
    "    'ProcessID', # object - A semi-unique (unique between currently running processes AND LogHost) value that identifies the process. ProcessID allows you to correlate other events logged in association with the same process through to the process end.\n",
    "    'ParentProcessName', # object - The process executable that started the new process. ParentProcessNames often do not have file extensions like ProcessName but can be compared by removing file extensions from the name.\n",
    "    'ParentProcessID', # object - Identifies the exact process that started the new process. Look for a preceding event 4688 with a ProcessID that matches this ParentProcessID.\n",
    "    'UserNameDomainName', # object - n'existe pas dans les fichiers, concaténation de UserName et DomainName \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FieldRecord():\n",
    "    \"\"\"classe pour gérer les champs. Les valeurs sont apprises au fil de l'eau, et une valeur 'categorical' est associée à chaque valeur unique.\n",
    "    \n",
    "    - list_values (type list) est la liste des valeurs apprises, que peut prendre le champ. Valeur initiale : [None]\n",
    "    - list_counts (type list) est la liste des nombres d'occurences de la valeur correpondante, à date. Valeur initiale : [0]\n",
    "    - nom : le nom du champ\n",
    "    - des méthodes utilitaires\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nom):\n",
    "        self.nom = nom # nom du champ...\n",
    "        self.list_values = [None] # valeurs possibles que peut prendre le champ. On initialise None à la valeur catgéorical 0\n",
    "        self.list_counts = [0] # nombres d'occurences constatées de la valeur correspondante\n",
    "           \n",
    "    def __str__(self):\n",
    "        ctr = sum(x for x in self.list_counts)\n",
    "        msg = f'Objet FieldRecord pour champ {self.nom} \\n - connaît {len(self.list_values)} valeurs distinctes \\n - a vu {ctr} champs au total'\n",
    "        return msg\n",
    "    \n",
    "    def __repr__(self):\n",
    "        ctr = sum(x for x in self.list_counts)\n",
    "        msg = f'Objet FieldRecord pour champ {self.nom} \\n - connaît {len(self.list_values)} valeurs distinctes \\n - a vu {ctr} champs au total'\n",
    "        return msg\n",
    "    \n",
    "    def get_field_categorical_value(self, val):\n",
    "        \"\"\"retourne l'int pour encodage catégorical du champ passé en argument\n",
    "        NB : les NaN ou None sont encodés à 0 par défaut. (cf constructeur)\n",
    "\n",
    "        Args:\n",
    "            val (int ou string): valeur extraite de la ligne JSON\n",
    "        \"\"\"\n",
    "        if val not in self.list_values:\n",
    "            # si la valeur est nouvelle, rajoute à la liste des valeurs connues\n",
    "            self.list_values.append(val) \n",
    "            self.list_counts.append(1)\n",
    "            cat_val = self.list_values.index(val)\n",
    "        else:\n",
    "            # si la valeur est déjà connue, retourne son index dans la liste comme categorical value et incrémente le compteur\n",
    "            cat_val = self.list_values.index(val) \n",
    "            self.list_counts[cat_val] += 1\n",
    "            \n",
    "        return cat_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créé les 21+1 objets Field Records\n",
    "\n",
    "def get_fresh_dico():\n",
    "    \n",
    "    dico = {\n",
    "        field : FieldRecord(field) for field in list_fields\n",
    "    }\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = get_fresh_dico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats_dico(dico=dico):\n",
    "    \"\"\"affiche statistiques descriptives du grand répertoire des champs traités\n",
    "    \"\"\"\n",
    "\n",
    "    output = \"\"\n",
    "    \n",
    "    for field in list_fields:\n",
    "        ctr = sum(x for x in dico[field].list_counts)\n",
    "        output = output + f\"Champ {field}: valeurs distinctes apprises {len(dico[field].list_values)}, valeurs vues : {ctr}\\n\"\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Champ Time: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ EventID: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ LogHost: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ LogonType: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ LogonTypeDescription: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ UserName: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ DomainName: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ LogonID: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ SubjectUserName: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ SubjectDomainName: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ SubjectLogonID: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ Status: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ Source: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ ServiceName: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ Destination: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ AuthenticationPackage: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ FailureReason: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ ProcessName: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ ProcessID: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ ParentProcessName: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ ParentProcessID: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "Champ UserNameDomainName: valeurs distinctes apprises 1, valeurs vues : 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(display_stats_dico(dico))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/benjamin.deporte/Cyber/data/LALN_processed/wls_day-02.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m dict_for_df \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     field : [] \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m list_fields\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m dico \u001b[38;5;241m=\u001b[39m get_fresh_dico()\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_SAMPLES):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# lit lignes du fichier une à une et convertit en dict Python\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         line \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\n",
      "File \u001b[0;32m~/.conda/envs/cyber/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/benjamin.deporte/Cyber/data/LALN_processed/wls_day-02.json'"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# -------------- Test -------------\n",
    "# ---------------------------------\n",
    "\n",
    "dirpath = DIRPATH\n",
    "filename = 'wls_day-02.json'\n",
    "filepath = dirpath + filename\n",
    "\n",
    "N_SAMPLES = 20\n",
    "\n",
    "# 1ere DataFrame : valeurs brutes\n",
    "dict_for_df_raw = {\n",
    "    field : [] for field in list_fields\n",
    "}\n",
    "\n",
    "# 2e DataFrame : valeurs nettes\n",
    "dict_for_df = {\n",
    "    field : [] for field in list_fields\n",
    "}\n",
    "\n",
    "dico = get_fresh_dico()\n",
    "\n",
    "with open(filepath, 'r') as f:\n",
    "    for i in range(N_SAMPLES):\n",
    "        # lit lignes du fichier une à une et convertit en dict Python\n",
    "        line = f.readline()\n",
    "        obj_json = json.loads(line)\n",
    "        # calcule à la main le UserNameDoaminName\n",
    "        obj_json['UserNameDomainName'] = obj_json.get('UserName') + obj_json.get('DomainName')\n",
    "        # print(obj_json)\n",
    "        # trouve la valeur de chaque champ (éventuellement None) et traduit en categorical value suivant dictionnaire dico\n",
    "        for field in list_fields:\n",
    "            # print(f'{field} = {obj_json.get(field,None)}')\n",
    "            # print(dico[field])\n",
    "            val = obj_json.get(field, None)\n",
    "            dict_for_df_raw[field].append(val)\n",
    "            dict_for_df[field].append(dico[field].get_field_categorical_value(val))\n",
    "            \n",
    "        msg = display_stats_dico(dico)\n",
    "        \n",
    "        # if i%10==0:\n",
    "        #     print(msg + '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.DataFrame(dict_for_df_raw)\n",
    "\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_for_df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# -- PARKING LOT : code pour itérateur ----------\n",
    "\n",
    "# class LinesBatchIterator():\n",
    "#     \"\"\"iterateur pour retourner des batchs de taille BATCH_SIZE depuis le fichier json FILEPATH\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, filepath, batch_size=None):\n",
    "#         self.filepath = filepath\n",
    "#         if batch_size == None:\n",
    "#             self.batch_size = 10 # taille de batch par défaut pour debug\n",
    "#         else:\n",
    "#             self.batch_size = batch_size\n",
    "            \n",
    "#     def __iter__(self):\n",
    "#         return self\n",
    "    \n",
    "#     def __next__(self):\n",
    "#         \"\"\"retourne le prochain batch de taille 'batch_size'\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # 1ere DataFrame : valeurs brutes - POUR DEBUG\n",
    "#         dict_for_df_raw = {\n",
    "#             field : [] for field in list_fields\n",
    "#             }\n",
    "        \n",
    "#         dict_for_df = {\n",
    "#             field : [] for field in list_fields\n",
    "#             }\n",
    "        \n",
    "#         with open(filepath) as f:\n",
    "#             for i in range(self.batch_size):\n",
    "#                 # lit lignes du fichier une à une et convertit en dict Python\n",
    "#                 # print(i)\n",
    "#                 line = f.readline()\n",
    "#                 # si fin du fichier, on arrête\n",
    "#                 if line==\"\":\n",
    "#                     raise StopIteration\n",
    "#                 # sinon, on traite\n",
    "#                 obj_json = json.loads(line)\n",
    "#                 # calcule à la main le UserNameDoaminName\n",
    "#                 obj_json['UserNameDomainName'] = obj_json.get('UserName') + obj_json.get('DomainName')\n",
    "#                 # trouve la valeur de chaque champ (éventuellement None) et traduit en categorical value suivant dictionnaire dico\n",
    "#                 for field in list_fields:\n",
    "#                     val = obj_json.get(field, None)\n",
    "#                     dict_for_df_raw[field].append(val)\n",
    "#                     dict_for_df[field].append(dico[field].get_field_categorical_value(val))\n",
    "#             df = pd.DataFrame(dict_for_df)\n",
    "#             df_raw = pd.DataFrame(dict_for_df_raw)\n",
    "            \n",
    "#         return df_raw, df\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         msg = f\"objet batch iterator, fichier = {self.filepath}, batch_size={self.batch_size}\"\n",
    "#         return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirpath = '/home/benjamin/Folders_Python/Cyber/data/LALN_processed/'\n",
    "# filename = 'wls_day-02.json'\n",
    "# filepath = dirpath + filename\n",
    "\n",
    "# line_iterator = LinesBatchIterator(filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(line_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw, df = next(line_iterator)\n",
    "\n",
    "# df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw, df = next(line_iterator)\n",
    "\n",
    "# df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA sur un GROS fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choisit le fichier\n",
    "\n",
    "dirpath = '/home/benjamin/Folders_Python/Cyber/data/LALN_processed/'\n",
    "filename = 'wls_day-01_subset_10000.json'\n",
    "\n",
    "filepath = dirpath + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisations\n",
    "\n",
    "# 1ere DataFrame : valeurs brutes\n",
    "dict_for_df_raw = {\n",
    "    field : [] for field in list_fields\n",
    "}\n",
    "\n",
    "# 2e DataFrame : valeurs nettes\n",
    "dict_for_df = {\n",
    "    field : [] for field in list_fields\n",
    "}\n",
    "\n",
    "dico = get_fresh_dico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Processe {filename}\\n\")\n",
    "\n",
    "with open(file=filepath) as f:\n",
    "    ctr_lines = 0\n",
    "    line = f.readline()\n",
    "    while line != \"\":\n",
    "        ctr_lines += 1\n",
    "        print(f\"traite ligne {ctr_lines}\", end=\"\\r\", flush=True)\n",
    "        obj_json = json.loads(line)\n",
    "        # calcule à la main le UserNameDomainName\n",
    "        obj_json['UserNameDomainName'] = obj_json.get('UserName') + obj_json.get('DomainName')\n",
    "        # print(obj_json)\n",
    "        # trouve la valeur de chaque champ (éventuellement None) et traduit en categorical value suivant dictionnaire dico\n",
    "        for field in list_fields:\n",
    "            # print(f'{field} = {obj_json.get(field,None)}')\n",
    "            # print(dico[field])\n",
    "            val = obj_json.get(field, None)\n",
    "            dict_for_df_raw[field].append(val)\n",
    "            dict_for_df[field].append(dico[field].get_field_categorical_value(val))\n",
    "        line = f.readline()\n",
    "    print(f\"\\n\")\n",
    "    \n",
    "    # df_raw = pd.DataFrame(dict_for_df_raw)\n",
    "    # df = pd.DataFrame(dict_for_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(display_stats_dico(dico))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_for_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training VAE continu (inspiré tuto F Chollet Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling layer\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the latent variable encoding an input.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seed_generator = keras.random.SeedGenerator(42)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs  # log var plutôt que l'écart type\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "        # reparametrisation trick from original article Kingma and Welling, “Auto-Encoding Variational Bayes”, ICLR 2014\n",
    "        scale = 0.25 # manage l'amplitude du bruit dans le reparametrisation trick pour éviter le posterior collapse\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list_fields)  # nombre de features\n",
    "output_dim = input_dim\n",
    "latent_dim = 2  # choix pour affichage 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# basic MLP\n",
    "x = layers.Dense(128, activation=\"relu\")(encoder_inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "# output mean et log_var de la gaussienne\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "# sample de la gaussienne inférée par le MLP\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "# input : vecteur de l'espace latent\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "x = layers.Dense(32, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "decoder_outputs = layers.Dense(output_dim)(x)\n",
    "\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe VAE\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # encodeur vers l'espace latent\n",
    "        self.encoder = encoder\n",
    "        # décodeur depuis l'espace latent\n",
    "        self.decoder = decoder\n",
    "        # losses\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # une étape de forward pass, avec différentiation des losses\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)  # forward pass de l'encodeur\n",
    "            reconstruction = self.decoder(z)  # veceteur reconstruit depuis la variable latent samplée\n",
    "            # 1ere loss : erreur reconstruction entre la data et la reconstruction\n",
    "            reconstruction_loss = ops.mean(keras.losses.mean_squared_error(data, reconstruction)),  # norme L2 pour calculer l'erreur de reconstruction\n",
    "            # 2e loss : KL entre le posterior z|x appris par l'encodeur et N(0,I) cible\n",
    "            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "            # loss totale (ELBO)\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        # calcul des gradients\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display projection de 'data' dans le latent space\n",
    "\n",
    "def plot_label_clusters(vae, data, titre=None): #, labels):\n",
    "    \"\"\"Affiche en 2D la projection de data dans l'espace latent de vae\n",
    "\n",
    "    Args:\n",
    "        vae (_type_): modèle VAE\n",
    "        labels (_type_): np.array 2D des points à afficher\n",
    "\n",
    "    Returns:\n",
    "        fig, ax\n",
    "    \"\"\"\n",
    "    \n",
    "    LIM = 3.0\n",
    "    BANDWIDTH = 0.20 # taille du noyau gaussien pour KDE\n",
    "    NUM = 50 # nombre points pour affichage contour\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    \n",
    "    # calculate projection of data on the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data, verbose=0)\n",
    "\n",
    "    # fit un KDE pour afficher un contour\n",
    "    rng = np.random.RandomState(42)\n",
    "    kde = KernelDensity(kernel='gaussian',bandwidth=BANDWIDTH).fit(z_mean)\n",
    "    \n",
    "    # affiche plt.contour\n",
    "    levels = [ 0.10, 0.25, 0.50, 0.75, 0.90 ]\n",
    "    x = np.linspace( -LIM, +LIM, num=NUM)\n",
    "    y = np.linspace( -LIM, +LIM, num=NUM)\n",
    "    X, Y = np.meshgrid(x, y) # X a shape NUm x NUM\n",
    "    Z = np.zeros(shape=(NUM,NUM)) # ny,nx\n",
    "    for j in range(NUM): # ny\n",
    "        for k in range(NUM): # nx\n",
    "            xc = X[j,k]\n",
    "            yc = Y[j,k]\n",
    "            point = np.array([xc,yc]).reshape(-1,2)\n",
    "            Z[j,k] = np.exp(kde.score_samples(point))  # repasse aux probas depuis le log retourné par score_samples\n",
    "            # print(point)\n",
    "    cs = ax.contour(X,Y,Z,levels)\n",
    "    ax.clabel(cs, inline=True, fontsize=10)  # affiche log probas\n",
    "    \n",
    "    # trace nuage de points projetés dans l'espace latent\n",
    "    ax.scatter(z_mean[:, 0], z_mean[:, 1], marker='.') \n",
    "    ax.set_xlabel(\"z[0]\")\n",
    "    ax.set_xlim(left=-LIM, right=+LIM)\n",
    "    ax.set_ylabel(\"z[1]\")\n",
    "    ax.set_ylim(bottom=-LIM, top=+LIM)\n",
    "    \n",
    "    if titre==None:\n",
    "        titre = 'Espace latent VAE'\n",
    "    ax.set_title(titre)\n",
    "    ax.grid(True)\n",
    "        \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instancie modèle, prêt au training\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour training séquentiel\n",
    "\n",
    "# ---- batch -----\n",
    "BATCH_SIZE = 2000 # batch de lignes que l'on va lire par training\n",
    "\n",
    "# ---- choisit le fichier\n",
    "dirpath = '/home/benjamin/Folders_Python/Cyber/data/LALN_processed/'\n",
    "filename = 'wls_day-01_subset_100000.json'\n",
    "filepath = dirpath + filename\n",
    "\n",
    "# ---- outputs modèle\n",
    "list_outputs = ['kl_loss', 'loss', 'reconstruction_loss']\n",
    "\n",
    "# --- pour faire une video\n",
    "video_rep = '/home/benjamin/Folders_Python/Cyber/data/jpg_for_videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Boucle training\n",
    "\n",
    "# --- annonce la couleur\n",
    "print(f\"Processe {filename}\\n\")\n",
    "\n",
    "# --- boucle\n",
    "with open(file=filepath) as f:\n",
    "    \n",
    "    ctr_lines_total = 0\n",
    "    ctr_batch = 1\n",
    "    line = f.readline()\n",
    "    \n",
    "    while line != \"\":\n",
    "        print(f\"Process batch {ctr_batch} de {BATCH_SIZE} lignes\")\n",
    "        ctr_lines_in_batch = 0\n",
    "        \n",
    "        # --- inits pour le batch \n",
    "        # 1ere DataFrame : valeurs brutes (DEBUG ONLY)\n",
    "        # dict_for_df_raw = { field : [] for field in list_fields }\n",
    "        \n",
    "        # 2e DataFrame : valeurs nettes\n",
    "        dict_for_df = { field : [] for field in list_fields }\n",
    "        \n",
    "        # -- lit un batch, produit une DataFrame\n",
    "        while (line != \"\") and (ctr_lines_in_batch<BATCH_SIZE):\n",
    "            ctr_lines_in_batch += 1\n",
    "            \n",
    "            # - donne des nouvelles\n",
    "            print(f\"traite ligne {ctr_lines_in_batch} du batch {ctr_batch}\", end=\"\\r\", flush=True)\n",
    "            obj_json = json.loads(line)\n",
    "            \n",
    "            # - calcule à la main le UserNameDomainName\n",
    "            obj_json['UserNameDomainName'] = obj_json.get('UserName') + obj_json.get('DomainName')\n",
    "            \n",
    "            # - trouve la valeur de chaque champ (éventuellement None) et traduit en categorical value suivant dictionnaire dico\n",
    "            for field in list_fields:\n",
    "                val = obj_json.get(field, None)\n",
    "                # dict_for_df_raw[field].append(val)\n",
    "                dict_for_df[field].append(dico[field].get_field_categorical_value(val))\n",
    "                \n",
    "            # - ligne suivante\n",
    "            line = f.readline()\n",
    "            \n",
    "        df = pd.DataFrame(dict_for_df)\n",
    "        \n",
    "        # -- training du VAE avec le nouveau batch\n",
    "        points = df.to_numpy()\n",
    "        print(f'\\nentraîne VAE sur batch {ctr_batch} --------------')\n",
    "        \n",
    "        callback = tf.keras.callbacks.EarlyStopping(    # NB : il faut instancier le callback à chaque training\n",
    "            monitor=\"loss\",\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        \n",
    "        # scaling pour convergence VAE. Mais a priori différent par batch ! Hum.\n",
    "        s = StandardScaler()\n",
    "        points_red = s.fit_transform(points)\n",
    "        \n",
    "        history = vae.fit(points_red, epochs=1000, batch_size=32, callbacks=[callback], verbose=0)\n",
    "        for o in list_outputs:\n",
    "            val = history.history.get(o)[-1]\n",
    "            print(f'{o} : {val:.2f}')\n",
    "        \n",
    "        # -- affiche projection du batch dans l'espace latent \n",
    "        titre = f'Espace latent VAE - batch {ctr_batch}'\n",
    "        fig, ax = plot_label_clusters(vae, points_red, titre)\n",
    "        savefig = video_rep + filename + f'_batch_{ctr_batch}.png'\n",
    "        plt.savefig(savefig)\n",
    "        plt.show()\n",
    "        \n",
    "        # -- prochain batch\n",
    "        ctr_batch += 1\n",
    "        line = f.readline()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour faire une video :\n",
    "\n",
    "# ffmpeg -r 1  -f image2 -s 640x640 -i wls_day-01_subset_nnnn.json_batch_%d.png -vcodec libx264 -crf 15 -pix_fmt yuv420p video.mp4\n",
    "\n",
    "# -r : frames per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RESPONSIBILITIES_MAX = min(10,int(N / 100))\n",
    "print(f'Max = {N_RESPONSIBILITIES_MAX} components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpbgm = BayesianGaussianMixture(\n",
    "    n_components = N_RESPONSIBILITIES_MAX, # max number of components, will be infered by data\n",
    "    weight_concentration_prior_type = 'dirichlet_process',   # weight concentration prior is Dirchlet process : (approximate) infinite number of components\n",
    "    random_state = 42,\n",
    "    # reg_covar = 1e-6,\n",
    "    verbose = 3,\n",
    "    max_iter = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpbgm.fit(df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_number = [ x for x in range(N_RESPONSIBILITIES_MAX) ]\n",
    "responsibilities = sorted(list(dpbgm.weights_),reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "ax.bar(comp_number, responsibilities)\n",
    "ax.set_xlabel('component number')\n",
    "ax.set_ylabel('component weight')\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(f'Bayesian MM responsibilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get responsibilities per point\n",
    "probas = dpbgm.predict_proba(df.to_numpy())\n",
    "\n",
    "# get predicted label, and associated responsibility, per data point\n",
    "labels = np.argmax(probas, axis=1)\n",
    "certainty = np.max(probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D display of Bayesian GMM - projected points on PCA manifold\n",
    "# if CHOICE=='lineaire':\n",
    "#     pca = PCA(n_components=3)\n",
    "# else:\n",
    "#     pca = KernelPCA(n_components=3, kernel='rbf', gamma=5.0)\n",
    "    \n",
    "# X_red = pca.fit_transform(df)\n",
    "\n",
    "X_red = pca3d.transform(df)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# invoke color map\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "# create normalizing object to map labels values into color map\n",
    "vmin = np.min(certainty)\n",
    "vmax = np.max(certainty)\n",
    "norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax, clip=False)\n",
    "\n",
    "# instantiate 3d object\n",
    "p = ax.scatter(X_red[:,0], X_red[:,1], X_red[:,2], c=cmap(norm(certainty)), marker='o') # \n",
    "ax.set_title(f'Bayesian GMM responsibility of affected gaussian, \\nmixture of maximum {N_RESPONSIBILITIES_MAX} gaussians,\\n 3D PCA projection - {N} points')\n",
    "ax.set_xlabel('vp_1')\n",
    "ax.set_ylabel('vp_2')\n",
    "ax.set_zlabel('vp_3')\n",
    "\n",
    "# display color map\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "fig.colorbar(p, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D display of Bayesian GMM - projected points on PCA manifold\n",
    "# if CHOICE=='lineaire':\n",
    "#     pca = PCA(n_components=3)\n",
    "# else:\n",
    "#     pca = KernelPCA(n_components=3, kernel='rbf', gamma=5.0)\n",
    "    \n",
    "# X_red = pca.fit_transform(df)\n",
    "\n",
    "X_red = pca3d.transform(df)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# invoke color map\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "# create normalizing object to map labels values into color map\n",
    "vmin = np.min(labels)\n",
    "vmax = np.max(labels)\n",
    "# norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax, clip=False)\n",
    "\n",
    "# instantiate 3d object\n",
    "p = ax.scatter(X_red[:,0], X_red[:,1], X_red[:,2], c=labels, marker='o') # \n",
    "ax.set_title(f'Bayesian GMM labels of affected gaussian, \\nmixture of maximum {N_RESPONSIBILITIES_MAX} gaussians,\\n 3D PCA projection - {N} points')\n",
    "ax.set_xlabel('vp_1')\n",
    "ax.set_ylabel('vp_2')\n",
    "ax.set_zlabel('vp_3')\n",
    "\n",
    "# display color map\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "fig.colorbar(p, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_points = vae.encoder.predict(s.transform(points))[0]  # récupére les z_mean samplées dns l'espace latent de dim 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM(kernel='rbf', gamma='scale').fit(latent_points)  # classifier dans l'espace latent dim 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# fig, ax = display_dataset(points, labels, mus, covs, sigma_max)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    estimator=clf,\n",
    "    X=latent_points,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0,1,2,3,4,5],\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.scatter(latent_points[:,0], latent_points[:,1], marker='.')\n",
    "\n",
    "ax.set_title('OC SVM')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
