{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json # original json library\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import sklearn\n",
    "import shap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILENAME = '/home/benjamin/Folders_Python/Cyber/logs/logfile.log'\n",
    "LOG_FORMAT = '%(asctime)% -- %(name)s -- %(levelname)s -- %(message)s'\n",
    "# LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific logger for the module\n",
    "logger = logging.getLogger(__name__)   # creates specific logger for the module\n",
    "logger.setLevel(logging.DEBUG)    # entry level of messages from all handlers\n",
    "LOG_FORMAT = '%(asctime)s -- %(name)s -- %(levelname)s -- %(message)s'\n",
    "formatter = logging.Formatter(LOG_FORMAT)\n",
    "\n",
    "# file handler to log everything\n",
    "file_handler = logging.FileHandler(LOG_FILENAME, mode='w')\n",
    "file_handler.setLevel(logging.INFO)  # all messages (DEBUG and up) get logged in the file\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# stream handler to show messages to the console\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.WARNING)  # Warning messages and up get displayed to the console\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)\n",
    "\n",
    "# start your engine\n",
    "logger.info(\"-------- new run --------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pcap file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB : tshark -r <file>.pcap -T json > <file_pcap>.json -t r\n",
    "# commande shell qui prend un pcap et le passe en json\n",
    "\n",
    "!rm /home/benjamin/Folders_Python/Cyber/data/outputs/input_pcap.json\n",
    "# !tshark -r /home/benjamin/Folders_Python/Cyber/data/input_pcaps/input.pcap -T json -t r > /home/benjamin/Folders_Python/Cyber/data/outputs/input_pcap.json\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#-- NB : On passe par le JSON depuis le pcap pour parser les paquets et créer la df 'raw packets' \n",
    "#-- A RE ECRIRE CAR ABSOLUMENT PAS OPTIMAL !!  UTILISER pyshark\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# logger.info(\"run tshark to create json translation of input.pcap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Packet():\n",
    "    \"\"\"Utility self-made unperfect class to parse the json object and extract features from a packet-like dict\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, raw_packet:dict) -> None:\n",
    "        self.raw_packet = raw_packet\n",
    "        self._packet_data = None\n",
    "        # logger.debug('constructor of Packet instance has finished')\n",
    "        \n",
    "    @property\n",
    "    def packet_data(self):\n",
    "        # returns the full dictionnary of features\n",
    "        if self._packet_data is not None:\n",
    "            return self._packet_data\n",
    "        else:\n",
    "            sl = self.raw_packet.get('_source').get('layers')\n",
    "            slf = sl.get('frame')\n",
    "            sle = sl.get('eth')\n",
    "            sli = sl.get('ip', {})  # return empty dict as default not found value so it can handle another get method\n",
    "            slu = sl.get('udp', {})\n",
    "            slt = sl.get('tcp', {})\n",
    "                                               \n",
    "            self._packet_data = {\n",
    "                'frame_time' : slf.get('frame.time'),\n",
    "                'frame_time_relative' : slf.get('frame.time_relative'),\n",
    "                'frame_length' : slf.get(\"frame.len\"),\n",
    "                'frame_protocols' : slf.get(\"frame.protocols\"),\n",
    "                'eth_source': sle.get(\"eth.src\"),\n",
    "                'eth_dest': sle.get(\"eth.dst\") ,\n",
    "                'ip_version': sli.get(\"ip.version\"),\n",
    "                'ip_header_length': sli.get(\"ip.hdr_len\"),\n",
    "                'ip_length': sli.get(\"ip.len\"),\n",
    "                'ip_id': sli.get(\"ip.id\"),\n",
    "                'ip_flags': sli.get(\"ip.flags\"),\n",
    "                'ip_ttl': sli.get(\"ip.ttl\"),\n",
    "                'ip_proto': sli.get(\"ip.proto\"),\n",
    "                'ip_source': sli.get(\"ip.src\"),\n",
    "                'ip_dest': sli.get(\"ip.dst\"),\n",
    "                'udp_source_port': slu.get(\"udp.srcport\"),\n",
    "                'udp_dest_port': slu.get(\"udp.port\"),\n",
    "                'udp_length': slu.get(\"udp.length\"),\n",
    "                'tcp_source_port': slt.get(\"tcp.srcport\"),\n",
    "                'tcp_dest_port': slt.get(\"tcp.dstport\"),\n",
    "                'tcp_length': slt.get(\"tcp.len\"),\n",
    "                'tcp_flags': slt.get(\"tcp.flags\"),\n",
    "            }\n",
    "            # logger.debug('packet_data @property method has finished')\n",
    "            return self._packet_data\n",
    "        \n",
    "    @packet_data.setter\n",
    "    def packet_data(self, input):\n",
    "        \"\"\"illegal attempt to write packet_data\"\"\"\n",
    "        logger.warning('Illegal attempt to write a data_packet in a packet object')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAP_FILENAME = \"/home/benjamin/Folders_Python/Cyber/data/outputs/input_pcap.json\"\n",
    "\n",
    "# with open (PCAP_FILENAME, errors='replace') as raw_packets:  # NB : errors='replace' bypasses decoding errors\n",
    "#     json_object = json.load(raw_packets)    # load le fichier json dans une structure Python (list of dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : premier dict de la liste : c'est un paquet (=une frame Ethernet)\n",
    "\n",
    "# json_object[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple d'instanciation d'un objet Packet\n",
    "# p = Packet(json_object[0])\n",
    "\n",
    "# p.packet_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce DataFrame for Raw Packets analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créé la liste de dictionnaires des data des objets Packets\n",
    "# packets = [ Packet(d).packet_data for d in json_object ]\n",
    "\n",
    "# df_packets = pd.DataFrame(packets)\n",
    "\n",
    "# df_packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_packets.describe(include='all').transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVE JSON Output by Suricata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Suricata to produce an eve.json file with alerts\n",
    "\n",
    "!rm /home/benjamin/Folders_Python/Cyber/data/outputs/eve.json\n",
    "!suricata -r /home/benjamin/Folders_Python/Cyber/data/input_pcaps/input.pcap -l /home/benjamin/Folders_Python/Cyber/data/outputs -k none\n",
    "# !suricata -r /home/benjamin/Folders_Python/Cyber/data/input_pcaps/bigFlows.pcap -l /home/benjamin/Folders_Python/Cyber/data/outputs -k none\n",
    "\n",
    "logger.info(\"run Suricata to reassemble flows and create alert logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas provides a useful method – json_normalize – for normalizing nested JSON fields into dataframe. Resulting columns use dot notation to signify nested objects, similar to how Elasticsearch does it\n",
    "\n",
    "SURICATA_EVE_LOG = \"/home/benjamin/Folders_Python/Cyber/data/outputs/eve.json\"\n",
    "\n",
    "with open (SURICATA_EVE_LOG) as packets:\n",
    "    df_log = pd.json_normalize(\n",
    "        [json.loads(packet) for packet in packets],\n",
    "        max_level=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log[df_log['event_type']=='flow']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce DataFrame for Flow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Suricata doc :\n",
    "\n",
    "# 15.1.2.12. Event type: Flow\n",
    "# 15.1.2.12.1. Fields\n",
    "\n",
    "#     “pkts_toserver”: total number of packets to server, include bypassed packets\n",
    "#     “pkts_toclient”: total number of packets to client\n",
    "#     “bytes_toserver”: total bytes count to server\n",
    "#     “bytes_toclient”: total bytes count to client\n",
    "#     “bypassed.pkts_toserver”: number of bypassed packets to server\n",
    "#     “bypassed.pkts_toclient”: number of bypassed packets to client\n",
    "#     “bypassed.bytes_toserver”: bypassed bytes count to server\n",
    "#     “bypassed.bytes_toclient”: bypassed bytes count to client\n",
    "#     “start”: date of start of the flow\n",
    "#     “end”: date of end of flow (last seen packet)\n",
    "#     “age”: duration of the flow\n",
    "#     “bypass”: if the flow has been bypassed, it is set to “local” (internal bypass) or “capture”\n",
    "#     “state”: display state of the flow (include “new”, “established”, “closed”, “bypassed”)\n",
    "#     “reason”: mechanism that did trigger the end of the flow (include “timeout”, “forced” and “shutdown”)\n",
    "#     “alerted”: “true” or “false” depending if an alert has been seen on flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.stamus-networks.com/blog/jupyter-playbooks-for-suricata-part-1\n",
    "\n",
    "# https://malware-traffic-analysis.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow():\n",
    "    \"\"\"Utility class - takes a event-flow string out of eve.json,\n",
    "       creates a one-level dict structure, suitable for dataframe creation\n",
    "    \"\"\"\n",
    "    def __init__(self, flow_event:dict):\n",
    "        if flow_event.get('event_type') != 'flow':\n",
    "            logger.critical(\"Attempt to build a Flow instance with a non-flow event\")\n",
    "            raise ValueError\n",
    "        self._raw_flow_event = flow_event\n",
    "        self._features = None\n",
    "        \n",
    "    @property\n",
    "    def features(self):\n",
    "        if self._features is not None:\n",
    "            return self._features\n",
    "        else:\n",
    "            keys_list_first_level = [\n",
    "                'timestamp',\n",
    "                'flow_id',\n",
    "                'src_ip',\n",
    "                'src_port',\n",
    "                'dest_ip',\n",
    "                'dest_port',\n",
    "                'proto'\n",
    "            ]\n",
    "            keys_list_second_level = [\n",
    "                'pkts_toserver',\n",
    "                'pkts_toclient',\n",
    "                'bytes_toserver',\n",
    "                'bytes_toclient',\n",
    "                'start',\n",
    "                'end',\n",
    "                'age',\n",
    "                'state',\n",
    "                'reason',\n",
    "                'alerted'\n",
    "            ]\n",
    "            d1 = { k: self._raw_flow_event.get(k) for k in keys_list_first_level }\n",
    "            d2 = { k: self._raw_flow_event.get('flow').get(k) for k in keys_list_second_level }\n",
    "            self._features = { **d1, **d2 }\n",
    "            # logger.info(\"built a Flow features object\")\n",
    "            return self._features\n",
    "        \n",
    "    @features.setter\n",
    "    def features(self, input):\n",
    "        logger.critical(\"illegal attempt to hard write features in a Flow object\")\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return json.dumps(self.features, indent=4)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return json.dumps(self.features, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple de flow JSON = \n",
    "# {\n",
    "# \"timestamp\":\"2023-06-17T10:46:05.765744+0200\",\n",
    "# \"flow_id\":860724109937755,\n",
    "# \"event_type\":\"flow\",\n",
    "# \"src_ip\":\"2a01:cb19:872e:3000:0e4f:3187:540c:d66c\",\n",
    "# \"src_port\":47864,\n",
    "# \"dest_ip\":\"2a00:1450:4007:081a:0000:0000:0000:2003\",\n",
    "# \"dest_port\":80,\n",
    "# \"proto\":\"TCP\",\n",
    "# \"flow\":\n",
    "#     {\"pkts_toserver\":6,\n",
    "#     \"pkts_toclient\":5,\n",
    "#     \"bytes_toserver\":516,\n",
    "#     \"bytes_toclient\":430,\n",
    "#     \"start\":\"2023-06-17T10:46:10.625755+0200\",\n",
    "#     \"end\":\"2023-06-17T10:46:44.150502+0200\",\n",
    "#     \"age\":34,\n",
    "#     \"state\":\"new\",\n",
    "#     \"reason\":\"shutdown\",\n",
    "#     \"alerted\":true},\n",
    "# \"community_id\":\"1:uRhWV544zvWeIohZCmryZHXZ5EA=\",\n",
    "# \"tcp\":\n",
    "#     {\"tcp_flags\":\"00\",\n",
    "#     \"tcp_flags_ts\":\"00\",\n",
    "#     \"tcp_flags_tc\":\"00\"\n",
    "#     }\n",
    "# }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURICATA_EVE_LOG = \"/home/benjamin/Folders_Python/Cyber/data/outputs/eve.json\"\n",
    "\n",
    "i=0\n",
    "columns_names =  [\n",
    "                'timestamp',\n",
    "                'flow_id',\n",
    "                'src_ip',\n",
    "                'src_port',\n",
    "                'dest_ip',\n",
    "                'dest_port',\n",
    "                'proto'\n",
    "            ] + [\n",
    "                'pkts_toserver',\n",
    "                'pkts_toclient',\n",
    "                'bytes_toserver',\n",
    "                'bytes_toclient',\n",
    "                'start',\n",
    "                'end',\n",
    "                'age',\n",
    "                'state',\n",
    "                'reason',\n",
    "                'alerted'\n",
    "            ]\n",
    "dict_for_dataframe = { k:[] for k in columns_names }\n",
    "\n",
    "with open (SURICATA_EVE_LOG) as f:\n",
    "    for event_string in f:\n",
    "        python_object = json.loads(event_string)\n",
    "        if python_object.get('event_type')=='flow':\n",
    "            flow = Flow(python_object)\n",
    "            for k in columns_names:\n",
    "                if dict_for_dataframe[k] == []:\n",
    "                    dict_for_dataframe[k] = [flow.features.get(k)]\n",
    "                else:\n",
    "                    dict_for_dataframe[k].append(flow.features.get(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow = pd.DataFrame(data=dict_for_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_flow.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codage en ordinal\n",
    "\n",
    "df['src_ip_ord'], uniques_src_ip = pd.factorize(values=df['src_ip'])\n",
    "df['dest_ip_ord'], uniques_dest_ip = pd.factorize(values=df['dest_ip'])\n",
    "df['proto_ord'], uniques_proto = pd.factorize(values=df['proto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flows flagged as 'alerted' by Suricata\n",
    "\n",
    "df[df['alerted']==True]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = ['src_port', 'dest_port', 'pkts_toserver', 'pkts_toclient', 'bytes_toserver', 'bytes_toclient', 'age', 'src_ip_ord', 'dest_ip_ord', 'proto_ord', 'alerted']\n",
    "features_names = ['src_port', 'dest_port', 'pkts_toserver', 'pkts_toclient', 'bytes_toserver', 'bytes_toclient', 'age', 'src_ip_ord', 'dest_ip_ord', 'proto_ord']\n",
    "\n",
    "df_clean = df[columns_names].dropna()\n",
    "\n",
    "X = df_clean[features_names].to_numpy()\n",
    "y = df_clean['alerted'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)  # scikit learn trick : when 0 < n_components < 1, choose the number of components so that the explained variance is above 90%\n",
    "\n",
    "X_new = pca.fit_transform(X)\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "xs = X_new[:,0]\n",
    "ys = X_new[:,1]\n",
    "zs = X_new[:,2]\n",
    "\n",
    "colors = [ 'red' if c == True else 'blue' for c in list(y)]\n",
    "\n",
    "ax.scatter3D(xs, ys, zs, c=colors)\n",
    "ax.set_title(\"PCA sur smallFlows\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.9, random_state=42)\n",
    "train_idx, test_idx = next(sss.split(X, y))\n",
    "X_train = X[train_idx]\n",
    "X_test =X[test_idx]\n",
    "Y_train = y[train_idx]\n",
    "Y_test = y[test_idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification du split\n",
    "\n",
    "n_pos_full = len(df[df[\"alerted\"]==True])\n",
    "n_neg_full = len(df[df[\"alerted\"]==False])\n",
    "\n",
    "print(f\"Full dataset : there are {n_pos_full} alerts, and {n_neg_full} benign, ie {n_pos_full/(n_pos_full+n_neg_full)*100:.2f}%\")\n",
    "\n",
    "n_pos_train = np.sum(Y_train)\n",
    "n_neg_train = len(Y_train) - n_pos_train\n",
    "print(f\"Training dataset : {n_pos_train} pos, {len(Y_train)} total, {n_pos_train/(n_neg_train+n_pos_train)*100:.2f}%\")\n",
    "\n",
    "n_pos_test = np.sum(Y_test)\n",
    "n_neg_test = len(Y_test) - n_pos_test\n",
    "print(f\"Test dataset : {n_pos_test} pos, {len(Y_test)} total, {n_pos_test/(n_neg_test+n_pos_test)*100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of classifiers : Logistic Regression, SVM Linear, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = sklearn.linear_model.LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = sklearn.svm.SVC(kernel='linear', probability=True)\n",
    "clf_svm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = sklearn.ensemble.RandomForestClassifier()\n",
    "clf_rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic_Regression\" : clf_lr,\n",
    "    \"SVM_linear_kernel\" : clf_svm,\n",
    "    \"Random_Forest\" : clf_rf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Accuracy of {name} = {model.score(X_test, Y_test)*100:.2f}%\")\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    cf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "    # print(cf_matrix)\n",
    "\n",
    "    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "    group_counts = [ f'{value:0}' for value in cf_matrix.flatten()]\n",
    "    group_percentages = [f'{value*100:.0f}%' for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [ f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages) ]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"coolwarm\", center = np.sum(cf_matrix)/2, xticklabels=[\"Pred Neg\", \"Pred Pos\"], yticklabels=[\"Real Neg\", \"Real Pos\"], ax=ax[0])\n",
    "    ax[0].set_title(name)\n",
    "\n",
    "    Y_scores = cross_val_predict(clf_lr, X_test, Y_test, cv=3, method=\"predict_proba\")\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, Y_scores[:,1])\n",
    "    \n",
    "    roc_curve_params[name] = [ fpr, tpr, thresholds ]\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "\n",
    "    ax[1].plot(fpr, tpr, linewidth=2)\n",
    "    ax[1].plot([0,1], [0,1], 'k--')\n",
    "    ax[1].set_xlabel(\"False Positive Rate\")\n",
    "    ax[1].set_ylabel(\"True Positive Rate (Recall) (Detection Rate)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "    \n",
    "ax.plot([0,1], [0,1], 'k--')\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate (Recall) (Detection Rate)\")\n",
    "    \n",
    "for name, list_params in roc_curve_params.items():\n",
    "    ax.plot(list_params[0], list_params[1], linewidth=2, label=name)\n",
    "\n",
    "ax.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression of the eigen vectors in the features space\n",
    "\n",
    "pca.components_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "explainer = shap.KernelExplainer(clf_rf.predict_proba, X_test[:100])\n",
    "shap_values = explainer.shap_values(X_test[:100])\n",
    "# shap.force_plot(explainer.expected_value[0], shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, feature_names=features_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
